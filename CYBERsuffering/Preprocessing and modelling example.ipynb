{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "from gensim.models import Phrases, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#относительно хороший результат чистки корпуса\n",
    "stopwords = ['б', 'г', 'д', 'е', 'ё', 'ж', 'з', 'й', 'л', 'м', 'н', 'п', 'р', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'гг']\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "for i, file in enumerate(os.listdir('C:\\\\Users\\\\')):\n",
    "    jj = os.path.abspath('Desktop\\\\Cybercorpora\\\\' + file)\n",
    "    dd = re.sub(r'\\\\', '//', jj)\n",
    "    xx = open(jj, 'r', encoding = 'utf-8')\n",
    "    ii = xx.read()  \n",
    "    ii = ii.lower()\n",
    "    jjj = tokenizer.tokenize(ii)\n",
    "    lst = []\n",
    "    for tryingtoclean in jjj:\n",
    "        tryingtoclean = re.sub('(данные об авторе)(\\.)?.*', '', tryingtoclean, flags=re.S)\n",
    "        tryingtoclean = re.sub(r'[a-zA-z]', '', tryingtoclean)\n",
    "        tryingtoclean = re.sub(r'_', '', tryingtoclean)\n",
    "        tryingtoclean = re.sub(r'\\d', '', tryingtoclean)\n",
    "        tryingtoclean = re.sub(r'[ñâÿçàíšßãóìøþôõýæùúñî¹áîřðûɫɥɨêɜǎäiöüéɨïіïëèåòè]', '', tryingtoclean)\n",
    "        words = re.findall(r'\\w+', tryingtoclean)\n",
    "        clean = filter(lambda a: a not in stopwords, words)\n",
    "        x = ' '.join(clean).rstrip()\n",
    "        lst.append(x)\n",
    "    xxx = '. '.join(lst).rstrip() + '.'\n",
    "    dotpattern = re.compile(r'(\\. \\.)+')\n",
    "    xxx = re.sub(dotpattern, '', xxx)\n",
    "    spacepattern = re.compile(r'(  )+')\n",
    "    xxx = re.sub(spacepattern, '', xxx)\n",
    "    dotpattern2 = re.compile(r'( \\.)+')\n",
    "    xxx = re.sub(dotpattern2, '', xxx)\n",
    "    dd = open(jj, 'w', encoding = 'utf-8')\n",
    "    charpattern = re.compile(r'(о о)+')\n",
    "    xxx = re.sub(charpattern, '', xxx)\n",
    "    charpattern2 = re.compile(r'(а а)+')\n",
    "    xxx = re.sub(charpattern2, '', xxx)\n",
    "    charpattern3 = re.compile(r'(с с)+')\n",
    "    xxx = re.sub(charpattern3, '', xxx)\n",
    "    dd.write(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for i, file in enumerate(os.listdir('C:\\\\Users\\\\')):\n",
    "    jj = os.path.abspath('Desktop\\\\Cybercorpora\\\\' + file)\n",
    "    dd = re.sub(r'\\\\', '//', jj)\n",
    "    xx = open(jj, 'r', encoding = 'utf-8')\n",
    "    ii = xx.read()  \n",
    "    files.append(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "sentence_stream = []\n",
    "for doc in files:\n",
    "    sents = doc.split(\".\")\n",
    "    for sent in sents:\n",
    "        sentence_stream.append(sent.split(' '))\n",
    "bigram = Phrases(sentence_stream, min_count=1, threshold=2)\n",
    "new_sents = []\n",
    "for sent in sentence_stream:\n",
    "    new_sents += bigram[sent]\n",
    "\n",
    "model = Word2Vec([new_sents], min_count=1)\n",
    "model.save('LinguisticModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x195b7e8ac50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
